{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataloader_CNN3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMR/0YdUAucO1OXdDmD1uVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrghassabi/Convolutional-Neural-Network/blob/main/dataloader_CNN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41EOUtoFAwXp"
      },
      "source": [
        "# DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzyVKD_z-i01"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "#load Dataset\n",
        "train_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=True, transform=transforms.ToTensor(),download=True)\n",
        "valid_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=False, transform=transforms.ToTensor(),download=True)\n",
        "test_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=False, transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "# #or apply different transforms to input data\n",
        "# train_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=True, transform=transforms.Compose([ \n",
        "#                                                                            transforms.Resize((14,14),transforms.InterpolationMode.BICUBIC),\n",
        "#                                                                            transforms.Grayscale(),\n",
        "#                                                                            transforms.ToTensor()]),\n",
        "#                                                                        download=True)\n",
        "# valid_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=False, transform=transforms.Compose([ \n",
        "#                                                                            transforms.Resize((14,14),transforms.InterpolationMode.BICUBIC),\n",
        "#                                                                            transforms.Grayscale(),\n",
        "#                                                                            transforms.ToTensor()]),\n",
        "#                                                                        download=True)\n",
        "# test_dataset=torchvision.datasets.MNIST(root='./Dataset/',train=False, transform=transforms.Compose([ \n",
        "#                                                                            transforms.Resize((14,14),transforms.InterpolationMode.BICUBIC),\n",
        "#                                                                            transforms.Grayscale(),\n",
        "#                                                                            transforms.ToTensor()]),\n",
        "#                                                                        download=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ukcEz9ZA64r"
      },
      "source": [
        "train_dataset.train_data  #to see train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7IXALlnBEv_"
      },
      "source": [
        "train_dataset.train_data.shape  #shape of all data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohdYuBXlBKCO"
      },
      "source": [
        "train_dataset.train_data[0,:,:].shape  #shape of one of data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F0WFhAEBSNm"
      },
      "source": [
        "# to show one image (requires opencv)\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "cv2.imshow('image',np.asarray(train_dataset.train_data[0,:,:]))\n",
        "cv2.waitkey(2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPwpiP_jC7DE"
      },
      "source": [
        "train_dataset.train_labels  #to see labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXW7yFvcLG8a"
      },
      "source": [
        "#parameters\n",
        "batch_size=64\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPZHq84rMmeZ"
      },
      "source": [
        "#data loader\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader=torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader=torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEjF6eGJM8Qg"
      },
      "source": [
        "#main loop\n",
        "for data,label in train_loader:  #to show batches with 64 size\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fizmCC4UXoo4"
      },
      "source": [
        "#main loop\n",
        "for imgs,label in train_loader:  #to show batches with 64 size\n",
        "    print(imgs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEOEvx3INLx1"
      },
      "source": [
        "data.shape  #pay attention that dimension is 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjMUyfBFO_Os"
      },
      "source": [
        "#load custom dataset, use Imagefolder or DatasetsFolder\n",
        "#this gives you address of images in the root folder\n",
        "#when you use dataloader, you will get access to images (e.x. 64 images with random idx)\n",
        "#train_dataset=torchvision.datasets.DatasetFolder('',transforms=transforms.ToTensor())\n",
        "#validation_dataset=torchvision.datasets.DatasetFolder('',transforms=transforms.ToTensor())\n",
        "#test_dataset=torchvision.datasets.DatasetFolder('',transforms=transforms.ToTensor())\n",
        "\n",
        "#train_dataset.samples\n",
        "#train_datset.classes\n",
        "#train_dataset.class_to_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh87lJ54W8Cs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvhr6gAhePi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiIkF7fFeVuK"
      },
      "source": [
        "# #convolutional neural network\n",
        "# # do not use this way\n",
        "\n",
        "# import torch.nn as nn\n",
        "# n_class=10\n",
        "# class convnet(nn.Module):\n",
        "#   def __init__(self):\n",
        "#       self.conv1=nn.Conv2d(1,16,3,1,2)  #in channel, out channel, filtersize, stride, padding\n",
        "#       self.BatchN1=nn.BatchNorm2d(16)\n",
        "#       self.relu1=nn.Relu()\n",
        "#       self.maxpl=nn.MaxPool2d(2,2)\n",
        "#       self.conv2=nn.Conv2d(16,32,3,1,2)  \n",
        "#       self.BatchN2=nn.BatchNorm2d(32)\n",
        "#       self.relu2=nn.Relu()\n",
        "#       self.maxp2=nn.MaxPool2d(2,2)\n",
        "#       self.fc=nn.Linear(7*7*32, n_class) \n",
        "#   def forward(self,x):\n",
        "#        a=self.conv1(x)\n",
        "#        a2=self.BatchN1N1(a)\n",
        "#        a3=self.relu1(a2)\n",
        "#        y=self.maxp1(a3)\n",
        "      \n",
        "#        return y\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC_6oCl4kQRh"
      },
      "source": [
        "import torch.nn as nn\n",
        "n_class=10\n",
        "\n",
        "\n",
        "\n",
        "class convnet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(convnet,self).__init__() \n",
        "    self.layer1=nn.Sequential(nn.Conv2d(1,16,3,1,2),   \n",
        "                                nn.BatchNorm2d(16),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(2,2))\n",
        "     \n",
        "    self.layer2=nn.Sequential(nn.Conv2d(16,32,3,1,2),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(2,2))\n",
        "     \n",
        "    \n",
        "    #self.fc=nn.Linear(8*8*32, 10) #10 is num_class  28by 28 is 8 by 8\n",
        "    #2 times pooling that split image size (1/2), first image is 15 by 15 (cuz we have padding)\n",
        "    #second image is 8 by 8. without padding, instead of 8 by 8 for FC, we have to put 7 by 7\n",
        "    self.fc=nn.Linear(8*8*self.layer2[0].out_channels, 10) #10 is num_class  28by 28 is 8 by 8\n",
        "  def forward(self,x):\n",
        "       out1=self.layer1(x)   #this like a dict\n",
        "       out2=self.layer2(out1)\n",
        "       out2=out2.reshape(out2.size(0),-1) #keep batch size of first dimention\n",
        "       y= self.fc(out2)\n",
        "       return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TRK8y0Xqw7_"
      },
      "source": [
        "convmodel=convnet() #build model\n",
        "print('*convmodel:\\n',convmodel)\n",
        "print('*convmodel.layer1:\\n',convmodel.layer1)\n",
        "print('*convmodel.layer1[0]:\\n',convmodel.layer1[0])\n",
        "#self.layer1[2] RELU()\n",
        "\n",
        "#out2.size(0)\n",
        "#out2.reshape(out2.size(0),-1).shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGz5jZSqyiEg"
      },
      "source": [
        "print(convmodel.layer2[3].kernel_size)\n",
        "print(convmodel.layer2[0].in_channels)\n",
        "print(convmodel.layer2[0].out_channels)\n",
        "print(convmodel.layer2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtjF-TFMlj9_"
      },
      "source": [
        "for imgs, lbls in train_loader:\n",
        "    out=convmodel(imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf1sIBTJyO5-"
      },
      "source": [
        "print(out[31])\n",
        "print(lbls[31])\n",
        "print(imgs[31].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcmj_1Tt_zQv"
      },
      "source": [
        "print(out[31][0])\n",
        "print(out[31][3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03dcSaIcIYaY"
      },
      "source": [
        "# train, validation,test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQQdZ8FrDLqn"
      },
      "source": [
        "# Params\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "#loss\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "lr=0.001\n",
        "optimizer_fn = torch.optim.Adam(convmodel.parameters(), lr=lr)\n",
        "#LR\n",
        "lr_sch=torch.optim.lr_scheduler.StepLR(optimizer_fn, 5,gamma=0.5) #adjust learning rate\n",
        "\n",
        "valid_num_steps=len(valid_loader)\n",
        "num_steps=len(train_loader)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    convmodel.train()\n",
        "    # here you can adjust learning rate\n",
        "    # if condition on d_loss < 0.2: #do this when STD of loss does not change\n",
        "    #    lr_sch.step() #here count, after 5 times, it changes LR\n",
        "    #print(lr_sch.get_lr())\n",
        "    for j, (imgs, lbls) in enumerate(train_loader):\n",
        "        out = convmodel(imgs)\n",
        "        loss_val = loss_fn(out, lbls)\n",
        "        optimizer_fn.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer_fn.step()\n",
        "        # if (j+1) % 2 == 0:\n",
        "        #     print('Train, Epoch [{}/{}] Step [{}/{}] Loss: {:.4f}'.\n",
        "        #           format(i+1, num_epochs, j+1, num_steps, loss_val.item()))\n",
        "        # if j == 10:\n",
        "        #     break\n",
        "    convmodel.eval()\n",
        "    corrects = 0\n",
        "    for k, (imgs, lbls) in enumerate(valid_loader):\n",
        "        out = convmodel(imgs)\n",
        "        loss_val = loss_fn(out, lbls)\n",
        "        predicted = torch.argmax(out, 1)\n",
        "        corrects += torch.sum(predicted == lbls)\n",
        "        # print('Validation, Step [{}/{}] Loss: {:.4f} Acc: {:.4f} '.format(k + 1, valid_num_steps, loss_val.item(), 100. * corrects / ((k + 1) * batch_size)))\n",
        "\n",
        "convmodel.eval()\n",
        "corrects = 0\n",
        "num_steps = len(test_loader)\n",
        "for j, (imgs, lbls) in enumerate(test_loader):\n",
        "    out = convmodel(imgs)\n",
        "    predicted = torch.argmax(out, 1)\n",
        "    corrects += torch.sum(predicted == lbls)\n",
        "    print('Step [{}/{}] Acc {:.4f}: '.format(j+1, num_steps, 100.*corrects/((j+1)*batch_size)))\n",
        "\n",
        "torch.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsgsumWuZqHO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}